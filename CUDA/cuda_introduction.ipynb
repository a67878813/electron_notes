{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits of GPU\n",
    "\n",
    "Devoting more transistors to data processing, for example,\n",
    "floating-point computations, is beneficial for highly parallel computations;\n",
    "the GPU can hide memory access latencies with computation, instead of relying on large data caches and comples flow control to avoid long memory access latencies, both of which are expensive in term of transistors.\n",
    "\n",
    "in general , an application has a mix of parallel parts and sequential parts, sy system are designed with a mix of GPUs and CPUs in order to maximize overall performance.\n",
    "Applications with a  high degree of parallelism can exploit this massively parallel nature of the GPU to achieve higher performance than on the CPU\n",
    "\n",
    ".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Scalable Programming Model\n",
    "\n",
    "The advent of multicore CPUs and manycore GPUs means that mainstream processor chips are now parallel systems.The challenge is to develop application software that transparently scales it parallelism to leverage the increasing number of processor cores, much as 3D graphic applications transparently scale their parallelism to manycore GPUs with widely varying numbers of cores.\n",
    "\n",
    "The CUDA parallel programming model is designed to overcome this challenge while maintaining a low learning curve for programmers familiar with standard programming languages such a sC.\n",
    "\n",
    "At its core are three key abstractions - a hierarchy of thread groups, shared memories, and barrier synchronization-\n",
    "that are simply exposed to the programmer as a minimal set of language extensions.\n",
    "\n",
    "These abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism.\n",
    "They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each subproblem into finer pieces that can be solved cooperatively in parallel by all threads within the block.\n",
    "\n",
    "This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability.\n",
    "Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially,\n",
    "so that a compiled CUDA program cna excute on any number of multiprocessors as illustrated by PIC 3,\n",
    "and only the runtime system needs to know the physical multiprocessor count.\n",
    "\n",
    "This scalable programming model allows the GPU architecture to span a wide market range by simply scaling the number of multiprocessors and memory partitions: from the high-performance enthusiast Gpu, adn professional Quadro &Tesla computing products to a variety of inexpensive, mainstream Geforce GPU.\n",
    "\n",
    "A GPU is built around an array of Streaming Multiprocessors(SMs).\n",
    "A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuda programming Model\n",
    "\n",
    "This chapter introduces the main concepts behind the CUDA programming model by outlining how they are exposed in C++.\n",
    "\n",
    "An extensive description of CUDA C++ is given in Programming Interface.\n",
    "\n",
    "2.1 Kernels \n",
    "\n",
    "CUDA C++ extends C== by allowing the programmer to define C++ functions, called kernels, that, when called, are executed N times in parallel by N different CUDA threas, as opposed to only once like regular C++ functions.\n",
    "\n",
    "A kernel is defined using the __global__ declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new <<<>>> execution contiguration syntax.\n",
    "\n",
    "Each thread that executes the kernel is given a unique *thread ID* that is **accessible within the kernel** through built-in variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustration, the following sample code, using the built-in variable *threadIdx*, adds two vectors A and B of size N and stores the result into vector C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Kernel definition\n",
    "__global__ void VecAdd(float* A, float* B, float* C)\n",
    "{\n",
    "    int i = threadIdx.x;\n",
    "    C[i] = A[i] + B[i];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    ...\n",
    "    // Kernel invocation with N threads\n",
    "    VecAdd<<<1, N>>>(A, B, C);\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Thread Hierarchy \n",
    "\n",
    "For convenience ,*threadIdx* is a 3-component vector,s othat threads can be identified using a one-dimensional, two-dimensional, or three- dimensional thread index, forming a one=dimensional ,two-dimensional ,or three-dimensional block of threads, called a **thread block**.\n",
    "This provides a natual way to invoke computation across the elements in a domain such as a vector, matrix, or volume.\n",
    "\n",
    "The index of a thread and its thread ID relate to each other in a straight forward way: \n",
    "\n",
    "For a one-dimensional block ,they are the same;\n",
    "\n",
    "for a two-dimensional block of size *Dx,Dy*,the thread ID of a thread of index\n",
    "(x,y) is *(x+y Dx*);\n",
    "\n",
    " the thread ID of a thread of index (x, y, z) is *(x + y Dx + z Dx Dy)*.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//As an example, the following code adds two matrices A and B of size NxN and stores the result into matrix C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Kernel definition\n",
    "__global__ void MatAdd(float A[N][N], float B[N][N],\n",
    "                       float C[N][N])\n",
    "{\n",
    "    int i = threadIdx.x;\n",
    "    int j = threadIdx.y;\n",
    "    C[i][j] = A[i][j] + B[i][j];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    ...\n",
    "    // Kernel invocation with one block of N * N * 1 threads\n",
    "    int numBlocks = 1;\n",
    "    dim3 threadsPerBlock(N, N);\n",
    "    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Block , contains threads per block.\n",
    "\n",
    "total_threas = Block* threads per block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a limit to the number of threads per block, \n",
    "\n",
    " since **all threads of a block** are expected to reside on the same *streaming multiprocessor core* \n",
    "and must share the limited memory resources of that core. \n",
    "\n",
    "On current GPUs, a thread block may contain up to 1024 threads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a *kernel* can be executed by multiple *equally-shaped thread blocks*, so that the total number of threads is equal to the number of threads per block times the number of blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blocks are organized into a one-dimensional, two-dimensional, or three-dimensional grid of thread blocks as illustrated by Figure 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the number of blocks(contain threads) in a grid is usually dictated by the size of the data being processed, which typically exceeds the number of processors in the system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The number of threads per block* and *the number of blocks per grid *specified in the <<<...>>> syntax can be of type int or dim3. Two-dimensional blocks or grids can be specified as in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each *block* within the *grid* can be identified by a one-dimensional, two-dimensional, or three-dimensional unique index accessible within the kernel through the built-in **blockIdx** variable. The dimension of the thread block is accessible within the kernel through the built-in **blockDim** variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the previous MatAdd() example to handle multiple blocks, the code becomes as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Kernel definition\n",
    "__global__ void MatAdd(float A[N][N], float B[N][N],\n",
    "float C[N][N])\n",
    "{\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int j = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    if (i < N && j < N)\n",
    "        C[i][j] = A[i][j] + B[i][j];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    ...\n",
    "    // Kernel invocation\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);\n",
    "    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);\n",
    "    ...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A thread block size of 16x16 (256 threads), although arbitrary in this case, is a common choice. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thread blocks are required to execute independently: It must be possible to execute them in any order, inparallel or in series.\n",
    "This independence requirement allows thread blocks to be scheduled in any order across any number of cores as illyustrated , enabling programmers to write code that scales with the number of cores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses.\n",
    "\n",
    "More precisely, one can specify synchronization points in the kernel by calling the *__syncthreads()* intrinsic function;\n",
    "\n",
    "//block 内同步锁\n",
    "*__syncthreads()* acts as a barrier at which all threads in the block must wait before any is allowed to proceed.\n",
    "\n",
    "Shared memory give an example of using shared memory.\n",
    "\n",
    "In addition to __syncthreads(), the Cooperative Groups API provides a rich set of thread-synchronization primitives.\n",
    "\n",
    "For efficient cooperation, the shared memory is expected to be a low-latency memory near each processor core( much like an L1 cache) and __syncthreads() is expected to be lightweeight.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A thread block cluster can be enabled in a kernel either using a compiler time kernel attribute using __cluster_dims__(X,Y,Z) or using the CUDA kernel launch API cudaLaunchKernelEx. The example below shows how to launch a cluster using compiler time kernel attribute. The cluster size using kernel attribute is fixed at compile time and then the kernel can be launched using the classical <<< , >>>. If a kernel uses compile-time cluster size, the cluster size *cannot* be modified when launching the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Kernel definition\n",
    "// Compile time cluster size 2 in X-dimension and 1 in Y and Z dimension\n",
    "__global__ void __cluster_dims__(2, 1, 1) cluster_kernel(float *input, float* output)\n",
    "{\n",
    "\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    float *input, *output;\n",
    "    // Kernel invocation with compile time cluster size\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);\n",
    "\n",
    "    // The grid dimension is not affected by cluster launch, and is still enumerated\n",
    "    // using number of blocks.\n",
    "    // The grid dimension must be a multiple of cluster size.\n",
    "    cluster_kernel<<<numBlocks, threadsPerBlock>>>(input, output);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A thread block cluster size can also be set at runtime and the kernel can be launched using the CUDA kernel launch API **cudaLaunchKernelEx**. The code example below shows how to launch a cluster kernel using the extensible API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Kernel definition\n",
    "// No compile time attribute attached to the kernel\n",
    "__global__ void cluster_kernel(float *input, float* output)\n",
    "{\n",
    "\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    float *input, *output;\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);\n",
    "\n",
    "    // Kernel invocation with runtime cluster size\n",
    "    {\n",
    "        cudaLaunchConfig_t config = {0};\n",
    "        // The grid dimension is not affected by cluster launch, and is still enumerated\n",
    "        // using number of blocks.\n",
    "        // The grid dimension should be a multiple of cluster size.\n",
    "        config.gridDim = numBlocks;\n",
    "        config.blockDim = threadsPerBlock;\n",
    "\n",
    "        cudaLaunchAttribute attribute[1];\n",
    "        attribute[0].id = cudaLaunchAttributeClusterDimension;\n",
    "        attribute[0].val.clusterDim.x = 2; // Cluster size in X-dimension\n",
    "        attribute[0].val.clusterDim.y = 1;\n",
    "        attribute[0].val.clusterDim.z = 1;\n",
    "        config.attrs = attribute;\n",
    "        config.numAttrs = 1;\n",
    "\n",
    "        cudaLaunchKernelEx(&config, cluster_kernel, input, output);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Memory Hierarchy\n",
    "\n",
    "CUDA threads may access data from multiple memory spaces during their execution\n",
    "\n",
    "Each thread has private local memory. \n",
    "\n",
    "\n",
    "Each thread block has shared memory visible to all threads of the block and with the same lifetime as the **block**.\n",
    "\n",
    "*Thread blocks* in a thread block **cluster** can perform read, write, and atomics operations on **each other’s shared memory**.\n",
    "\n",
    "\n",
    "here are also two additional read-only memory spaces accessible by all threads: \n",
    "\n",
    " the constant \n",
    " and texture memory spaces. \n",
    "\n",
    "\n",
    "The global, constant, and texture memory spaces are optimized for different memory usages \n",
    "\n",
    " Texture memory also offers different addressing modes, as well as data filtering, for some specific data formats \n",
    "\n",
    " The global,\n",
    "  constant,\n",
    "   and texture memory spaces are persistent **across kernel launches** by the same application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. Heterogeneous Programming\n",
    "\n",
    "the CUDA programming model assumes that the CUDA threads execute on a physically separate device that operates as a coprocessor to the host running the C++ program. This is the case, for example,\n",
    "\n",
    "when the kernels execute on a GPU and the rest of the C++ program executes on a CPU.\n",
    "\n",
    "The CUDA programming model also assumes \n",
    "that both the host and the device maintain their own separate memory spaces in DRAM, \n",
    "referred to as *host memory* and *device memory*, respectively. \n",
    "\n",
    "Therefore, a program manages the global, constant, and texture memory spaces visible to kernels through \n",
    "calls to the CUDA runtime\n",
    "\n",
    " This includes device memory *allocation* and *deallocation* as well as *data transfer* between host and device memory.\n",
    "\n",
    "Unified Memory provides managed memory to bridge the host and device memory spaces. \n",
    "\n",
    "Managed memory is accessible from all CPUs and GPUs in the system \n",
    "as a single, coherent memory image with a common address space. \n",
    "\n",
    " This capability enables oversubscription of device memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5. Asynchronous SIMT Programming Model\n",
    "\n",
    "In the CUDA programming model \n",
    "a thread is the lowest level of abstraction for doing a computation or a memory operation. \n",
    "\n",
    "Starting with devices based on the *NVIDIA Ampere* GPU architecture, the CUDA programming model provides acceleration to memory operations via the asynchronous programming model. \n",
    "\n",
    "The asynchronous programming model \n",
    "defines the behavior of *asynchronous* operations with respect to **CUDA threads**.\n",
    "\n",
    "The asynchronous programming model defines the behavior of *Asynchronous Barrier* for *synchronization* between CUDA threads. \n",
    "\n",
    "The model also explains and defines how **cuda::memcpy_async** can be used to move data asynchronously from global memory while **computing in the GPU**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5.1. Asynchronous Operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
