{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA events for both GPU timing and overlapping CPU and GPU execution. \n",
    "\n",
    "Events are inserted into a stream\n",
    " * of CUDA calls.  Since CUDA stream calls are asynchronous, the CPU can\n",
    " * perform computations while GPU is executing (including DMA memcopies\n",
    " * between the host and device).  CPU can query CUDA events to determine\n",
    " * whether GPU has completed tasks.\n",
    "\n",
    " Since CUDA stream calls are asynchronous, the CPU can\n",
    " * perform computations while GPU is executing (including DMA memcopies\n",
    " * between the host and device). \n",
    "\n",
    "// spin lock?\n",
    "  CPU can query CUDA events to determine\n",
    " * whether GPU has completed tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This will output the proper CUDA error strings \n",
    "in the event\n",
    "// that a CUDA host call returns an error\n",
    "#define checkCudaErrors(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cudaError_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//malloc 对\n",
    "\n",
    "  int n = 16 * 1024 * 1024;//16mb\n",
    "  int nbytes = n * sizeof(int);\n",
    "  int value = 26;\n",
    "\n",
    "  // allocate host memory\n",
    "  int *a = 0;\n",
    "  checkCudaErrors(cudaMallocHost((void **)&a, nbytes));\n",
    "  memset(a, 0, nbytes);\n",
    "\n",
    "  // allocate device memory\n",
    "  int *d_a = 0;\n",
    "  checkCudaErrors(cudaMalloc((void **)&d_a, nbytes));\n",
    "  checkCudaErrors(cudaMemset(d_a, 255, nbytes));\n",
    "\n",
    "\n",
    "//=============release resorces===================\n",
    "  checkCudaErrors(cudaFreeHost(a));\n",
    "  checkCudaErrors(cudaFree(d_a));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  // set kernel launch configuration\n",
    "  dim3 threads = dim3(512, 1);\n",
    "  dim3 blocks = dim3(n / threads.x, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "dim3\n",
    "\n",
    "stream 0 \n",
    "    events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//events 对\n",
    "\n",
    "//create cuda event  handles\n",
    "  // create cuda event handles\n",
    "  cudaEvent_t start, stop;\n",
    "  //checkCudaErrors(cudaEventCreate(&start));\n",
    "  //checkCudaErrors(cudaEventCreate(&stop));\n",
    "\n",
    "  //StopWatchInterface *timer = NULL;\n",
    "  //sdkCreateTimer(&timer);\n",
    "  //sdkResetTimer(&timer);\n",
    "\n",
    "  checkCudaErrors(cudaDeviceSynchronize());//block until completed all tasks\n",
    "  float gpu_time = 0.0f;\n",
    "\n",
    "\n",
    "  // asynchronously issue work to the GPU (all to stream 0)\n",
    "  checkCudaErrors(cudaProfilerStart());\n",
    "  //sdkStartTimer(&timer);\n",
    "\n",
    "\n",
    "  \n",
    "  cudaEventRecord(start, 0);\n",
    "      cudaMemcpyAsync(d_a, a, nbytes, cudaMemcpyHostToDevice, 0);\n",
    "      increment_kernel<<<blocks, threads, 0, 0>>>(d_a, value);\n",
    "      cudaMemcpyAsync(a, d_a, nbytes, cudaMemcpyDeviceToHost, 0);\n",
    "  cudaEventRecord(stop, 0);\n",
    "\n",
    "  //sdkStopTimer(&timer);\n",
    "  checkCudaErrors(cudaProfilerStop());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//自旋锁等待，处理其他事项\n",
    "  // have CPU do some work while waiting for stage 1 to finish\n",
    "  unsigned long int counter = 0;\n",
    "\n",
    "  while (cudaEventQuery(stop) == cudaErrorNotReady) {\n",
    "    counter++;\n",
    "  }\n",
    "\n",
    "  checkCudaErrors(cudaEventElapsedTime(&gpu_time, start, stop));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  // print the cpu and gpu times\n",
    "  printf(\"time spent executing by the GPU: %.2f ms\\n\", gpu_time);\n",
    "  printf(\"time spent by CPU in CUDA calls: %.2f ms\\n\", sdkGetTimerValue(&timer));\n",
    "  printf(\"CPU executed %lu iterations while waiting for GPU to finish\\n\",\n",
    "         counter);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  // check the output for correctness\n",
    "  bool bFinalResults = correct_output(a, n, value);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  // release resources\n",
    "  checkCudaErrors(cudaEventDestroy(start));\n",
    "  checkCudaErrors(cudaEventDestroy(stop));\n",
    "  checkCudaErrors(cudaFreeHost(a));\n",
    "  checkCudaErrors(cudaFree(d_a));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  exit(bFinalResults ? EXIT_SUCCESS : EXIT_FAILURE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
